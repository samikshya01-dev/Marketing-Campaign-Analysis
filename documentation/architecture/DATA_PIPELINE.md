# Data Pipeline Documentation

## ðŸ“Š End-to-End Data Pipeline Architecture

This document describes the complete data flow through the Marketing Campaign Analytics Platform.

---

## ðŸ”„ Pipeline Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MySQL    â”‚
â”‚  Database  â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â†“ Extract
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data      â”‚
â”‚  Loader    â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â†“ Transform
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data      â”‚
â”‚  Cleaner   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â†“ Analyze
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    EDA     â”‚  Customer  â”‚    ROI     â”‚
â”‚  Analyzer  â”‚  Segment.  â”‚ Calculator â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚            â”‚            â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â†“ Load
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Report Generation            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   PDF   â”‚  â”‚  HTML  â”‚  â”‚PowerBI â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“‹ Pipeline Stages

### Stage 1: Data Extraction (Extract)

**Component**: `DataLoader`  
**Duration**: ~2.5 seconds (100K records)  
**Input**: MySQL Database  
**Output**: Raw pandas DataFrames

#### Process Flow

```python
# 1. Database Connection
engine = create_engine(connection_string,
                      pool_size=10,
                      max_overflow=20)

# 2. Query Execution
campaigns_query = """
    SELECT campaign_name, channel, cost, impressions, clicks,
           conversions, revenue, date
    FROM campaigns
    WHERE date >= DATE_SUB(CURDATE(), INTERVAL 365 DAY)
    ORDER BY date DESC
"""

# 3. Data Loading
campaigns_df = pd.read_sql(campaigns_query, engine)
customers_df = pd.read_sql(customers_query, engine)

# 4. Initial Type Conversion
campaigns_df['date'] = pd.to_datetime(campaigns_df['date'])
```

#### Performance Optimization

**Database-Level**:
- Indexed columns for fast retrieval
- Date filtering at database level
- SELECT only needed columns
- Connection pooling

**Application-Level**:
- Batch loading for large datasets
- Lazy evaluation where possible
- Memory-efficient data types

#### Monitoring

```python
logger.info(f"Loaded {len(campaigns_df)} campaigns")
logger.info(f"Loaded {len(customers_df)} customers")
logger.info(f"Memory usage: {campaigns_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
```

---

### Stage 2: Data Transformation (Transform)

**Component**: `DataCleaner`  
**Duration**: ~1.8 seconds (100K records)  
**Input**: Raw DataFrames  
**Output**: Cleaned DataFrames

#### Transformation Steps

**Step 1: Data Validation**
```python
def validate_campaigns(df):
    # Required columns check
    required_cols = ['campaign_name', 'channel', 'cost', 'revenue']
    missing_cols = set(required_cols) - set(df.columns)
    if missing_cols:
        raise ValueError(f"Missing columns: {missing_cols}")
    
    # Data type validation
    assert df['cost'].dtype in ['float64', 'int64'], "Cost must be numeric"
    assert df['revenue'].dtype in ['float64', 'int64'], "Revenue must be numeric"
    
    # Business rule validation
    assert (df['cost'] >= 0).all(), "Cost cannot be negative"
    assert (df['revenue'] >= 0).all(), "Revenue cannot be negative"
    assert (df['clicks'] <= df['impressions']).all(), "Clicks cannot exceed impressions"
    
    return df
```

**Step 2: Missing Data Handling**
```python
def handle_missing_data(df):
    # Critical fields - Fail if missing
    if df[['campaign_name', 'cost', 'revenue']].isnull().any().any():
        raise ValueError("Critical fields have missing data")
    
    # Numeric fields - Fill with 0
    numeric_cols = ['impressions', 'clicks', 'conversions']
    df[numeric_cols] = df[numeric_cols].fillna(0)
    
    # Categorical fields - Fill with mode
    if df['channel'].isnull().any():
        df['channel'].fillna(df['channel'].mode()[0], inplace=True)
    
    logger.info(f"Missing data handled: {df.isnull().sum().sum()} nulls filled")
    return df
```

**Step 3: Duplicate Removal**
```python
def remove_duplicates(df):
    initial_count = len(df)
    df = df.drop_duplicates(subset=['campaign_name', 'date'], keep='first')
    duplicates_removed = initial_count - len(df)
    
    if duplicates_removed > 0:
        logger.warning(f"Removed {duplicates_removed} duplicate records")
    
    return df
```

**Step 4: Data Normalization**
```python
def normalize_data(df):
    # Standardize channel names
    channel_mapping = {
        'email': 'Email',
        'social': 'Social Media',
        'search': 'Paid Search',
        'display': 'Display',
        'mobile': 'Mobile Push'
    }
    df['channel'] = df['channel'].str.lower().map(channel_mapping)
    
    # Trim whitespace
    df['campaign_name'] = df['campaign_name'].str.strip()
    
    # Standardize date format
    df['date'] = pd.to_datetime(df['date']).dt.date
    
    return df
```

**Step 5: Outlier Detection & Handling**
```python
def detect_outliers(df, column, method='iqr'):
    if method == 'iqr':
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = (df[column] < lower_bound) | (df[column] > upper_bound)
        
        if outliers.any():
            logger.warning(f"Found {outliers.sum()} outliers in {column}")
            # Log outliers for review, don't remove automatically
            df['is_outlier'] = outliers
    
    return df
```

**Step 6: Feature Engineering**
```python
def create_derived_metrics(df):
    # Click-Through Rate (CTR)
    df['ctr'] = np.where(df['impressions'] > 0,
                         (df['clicks'] / df['impressions']) * 100,
                         0)
    
    # Conversion Rate
    df['conversion_rate'] = np.where(df['clicks'] > 0,
                                      (df['conversions'] / df['clicks']) * 100,
                                      0)
    
    # Cost Per Click (CPC)
    df['cpc'] = np.where(df['clicks'] > 0,
                          df['cost'] / df['clicks'],
                          0)
    
    # Cost Per Acquisition (CPA)
    df['cpa'] = np.where(df['conversions'] > 0,
                          df['cost'] / df['conversions'],
                          0)
    
    # Return on Ad Spend (ROAS)
    df['roas'] = np.where(df['cost'] > 0,
                           df['revenue'] / df['cost'],
                           0)
    
    # Return on Investment (ROI)
    df['roi'] = np.where(df['cost'] > 0,
                          ((df['revenue'] - df['cost']) / df['cost']) * 100,
                          0)
    
    # Profit
    df['profit'] = df['revenue'] - df['cost']
    
    return df
```

#### Data Quality Metrics

```python
quality_report = {
    'total_records': len(df),
    'completeness': (1 - df.isnull().sum().sum() / df.size) * 100,
    'duplicates_removed': duplicates_removed,
    'outliers_detected': outliers.sum(),
    'columns_created': len(derived_cols)
}
logger.info(f"Data quality: {quality_report}")
```

---

### Stage 3: Data Analysis (Analyze)

**Components**: `EDAAnalyzer`, `CustomerSegmentation`, `ROICalculator`  
**Duration**: ~8 seconds (100K records)  
**Input**: Cleaned DataFrames  
**Output**: Analysis results, visualizations, metrics

#### 3A. Exploratory Data Analysis

**Time Series Analysis**:
```python
# Revenue trend over time
daily_revenue = campaigns_df.groupby('date')['revenue'].sum()
plt.figure(figsize=(12, 6))
plt.plot(daily_revenue.index, daily_revenue.values)
plt.title('Revenue Trend Over Time')
plt.savefig('outputs/visualizations/revenue_trend.png')
```

**Channel Performance Analysis**:
```python
# Aggregate by channel
channel_metrics = campaigns_df.groupby('channel').agg({
    'cost': 'sum',
    'revenue': 'sum',
    'impressions': 'sum',
    'clicks': 'sum',
    'conversions': 'sum'
}).reset_index()

# Calculate metrics
channel_metrics['roi'] = ((channel_metrics['revenue'] - channel_metrics['cost']) / 
                          channel_metrics['cost'] * 100)
channel_metrics['ctr'] = (channel_metrics['clicks'] / channel_metrics['impressions'] * 100)
```

**Distribution Analysis**:
```python
# ROI distribution by channel
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Histogram
axes[0].hist(campaigns_df['roi'], bins=50, edgecolor='black')
axes[0].set_title('ROI Distribution')

# Box plot by channel
campaigns_df.boxplot(column='roi', by='channel', ax=axes[1])
axes[1].set_title('ROI by Channel')

plt.tight_layout()
plt.savefig('outputs/visualizations/roi_distribution.png')
```

#### 3B. Customer Segmentation

**Feature Preparation**:
```python
# Select features for clustering
features = ['age', 'sessions', 'avg_session_duration', 
            'pages_per_session', 'transactions', 'revenue']
X = customers_df[features]

# Handle missing values
X = X.fillna(X.mean())

# Feature scaling (CRITICAL for K-Means)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

**Optimal Cluster Selection**:
```python
# Elbow method
inertias = []
silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))

# Plot elbow curve
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(K_range, inertias, 'bx-')
plt.xlabel('k')
plt.ylabel('Inertia')
plt.title('Elbow Method')

plt.subplot(1, 2, 2)
plt.plot(K_range, silhouette_scores, 'rx-')
plt.xlabel('k')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score Method')

# Select optimal k (typically where curve bends)
optimal_k = 3  # Based on elbow point
```

**Clustering Execution**:
```python
# Train final model
kmeans = KMeans(n_clusters=optimal_k, 
                init='k-means++',
                n_init=10,
                max_iter=300,
                random_state=42)

clusters = kmeans.fit_predict(X_scaled)
customers_df['cluster'] = clusters

# Analyze cluster profiles
cluster_profiles = customers_df.groupby('cluster').agg({
    'age': ['mean', 'std'],
    'sessions': ['mean', 'sum'],
    'revenue': ['mean', 'sum', 'count']
})

logger.info(f"Customer segmentation complete: {optimal_k} clusters")
logger.info(f"\n{cluster_profiles}")
```

#### 3C. ROI Calculation

**Campaign-Level ROI**:
```python
roi_metrics = campaigns_df.copy()

# Financial metrics
roi_metrics['profit'] = roi_metrics['revenue'] - roi_metrics['cost']
roi_metrics['roi'] = (roi_metrics['profit'] / roi_metrics['cost']) * 100
roi_metrics['roas'] = roi_metrics['revenue'] / roi_metrics['cost']

# Performance metrics
roi_metrics['ctr'] = (roi_metrics['clicks'] / roi_metrics['impressions']) * 100
roi_metrics['conversion_rate'] = (roi_metrics['conversions'] / roi_metrics['clicks']) * 100
roi_metrics['cpc'] = roi_metrics['cost'] / roi_metrics['clicks']
roi_metrics['cpa'] = roi_metrics['cost'] / roi_metrics['conversions']
roi_metrics['conversion_value'] = roi_metrics['revenue'] / roi_metrics['conversions']
```

**Channel-Level Aggregation**:
```python
channel_summary = roi_metrics.groupby('channel').agg({
    'cost': 'sum',
    'revenue': 'sum',
    'profit': 'sum',
    'impressions': 'sum',
    'clicks': 'sum',
    'conversions': 'sum',
    'roi': 'mean',
    'roas': 'mean',
    'ctr': 'mean',
    'conversion_rate': 'mean'
}).round(2)

# Rank channels by ROI
channel_summary['roi_rank'] = channel_summary['roi'].rank(ascending=False)
channel_summary = channel_summary.sort_values('roi', ascending=False)
```

---

### Stage 4: Report Generation (Load)

**Component**: `ExecutiveSummaryGenerator`  
**Duration**: ~3 seconds  
**Input**: Analysis results  
**Output**: PDF, HTML, Power BI files

#### 4A. PDF Report Generation

```python
def generate_pdf_report():
    # 1. Create document
    doc = SimpleDocTemplate('executive_summary.pdf', pagesize=letter)
    elements = []
    
    # 2. Add header
    styles = getSampleStyleSheet()
    header_style = ParagraphStyle(
        'CustomHeader',
        parent=styles['Heading1'],
        fontSize=24,
        textColor=colors.HexColor('#2C3E50'),
        alignment=TA_CENTER
    )
    elements.append(Paragraph('Marketing Campaign Analysis', header_style))
    elements.append(Spacer(1, 0.5*inch))
    
    # 3. Add key metrics table
    metrics_data = [
        ['Metric', 'Value'],
        ['Total Cost', f'Rs. {total_cost:,.2f}'],
        ['Total Revenue', f'Rs. {total_revenue:,.2f}'],
        ['Total Profit', f'Rs. {total_profit:,.2f}'],
        ['Average ROI', f'{avg_roi:.2f}%'],
        ['Average ROAS', f'{avg_roas:.2f}x']
    ]
    
    metrics_table = Table(metrics_data, colWidths=[2*inch, 2*inch])
    metrics_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#3498DB')),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 14),
        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
        ('GRID', (0, 0), (-1, -1), 1, colors.black)
    ]))
    elements.append(metrics_table)
    
    # 4. Add channel performance table
    channel_data = [['Channel', 'Cost', 'Revenue', 'Profit', 'ROI']]
    for channel, row in channel_summary.iterrows():
        channel_data.append([
            channel,
            f"Rs. {row['cost']:,.2f}",
            f"Rs. {row['revenue']:,.2f}",
            f"Rs. {row['profit']:,.2f}",
            f"{row['roi']:.2f}%"
        ])
    
    channel_table = Table(channel_data, colWidths=[1.5*inch, 1.2*inch, 
                                                   1.2*inch, 1.2*inch, 1*inch])
    # ... styling ...
    elements.append(channel_table)
    
    # 5. Build PDF
    doc.build(elements)
    logger.info("PDF report generated successfully")
```

#### 4B. HTML Report Generation

```python
def generate_html_report():
    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Marketing Campaign Analysis</title>
        <style>
            body {{ font-family: Arial, sans-serif; }}
            .metric-card {{ background: #f8f9fa; padding: 20px; margin: 10px; }}
            table {{ border-collapse: collapse; width: 100%; }}
            th {{ background-color: #3498DB; color: white; padding: 12px; }}
            td {{ border: 1px solid #ddd; padding: 8px; }}
        </style>
    </head>
    <body>
        <h1>Marketing Campaign Analysis</h1>
        <div class="metrics">
            <div class="metric-card">
                <h3>Total Cost</h3>
                <p>Rs. {total_cost:,.2f}</p>
            </div>
            <!-- More metric cards -->
        </div>
        <table>
            <thead>
                <tr><th>Channel</th><th>Cost</th><th>Revenue</th><th>ROI</th></tr>
            </thead>
            <tbody>
                {channel_rows}
            </tbody>
        </table>
    </body>
    </html>
    """
    
    # Populate template
    html_content = html_template.format(
        total_cost=total_cost,
        total_revenue=total_revenue,
        channel_rows=generate_channel_rows(channel_summary)
    )
    
    # Write to file
    with open('executive_summary.html', 'w') as f:
        f.write(html_content)
```

#### 4C. Power BI Export

```python
def export_to_powerbi():
    # Export main campaign data
    campaigns_df.to_csv('outputs/dashboards/campaign_data_powerbi.csv', 
                        index=False)
    
    # Export channel summary
    channel_summary.to_csv('outputs/dashboards/channel_performance_powerbi.csv')
    
    # Export customer segments
    customers_df.to_csv('outputs/dashboards/customer_segments_powerbi.csv', 
                        index=False)
    
    # Export ROI analysis
    roi_metrics.to_csv('outputs/dashboards/roi_analysis_powerbi.csv', 
                       index=False)
    
    # Create metadata file
    metadata = {
        'generated_at': datetime.now().isoformat(),
        'campaigns_count': len(campaigns_df),
        'customers_count': len(customers_df),
        'date_range': f"{campaigns_df['date'].min()} to {campaigns_df['date'].max()}"
    }
    
    with open('outputs/dashboards/powerbi_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    
    logger.info("Power BI export completed")
```

---

## ðŸ” Pipeline Monitoring

### Execution Metrics

```python
pipeline_metrics = {
    'pipeline_id': str(uuid.uuid4()),
    'start_time': start_time,
    'end_time': end_time,
    'duration_seconds': (end_time - start_time).total_seconds(),
    'stages': {
        'extraction': {'duration': 2.5, 'records': 100000, 'status': 'success'},
        'transformation': {'duration': 1.8, 'records': 99500, 'status': 'success'},
        'analysis': {'duration': 8.0, 'status': 'success'},
        'reporting': {'duration': 3.1, 'status': 'success'}
    },
    'data_quality': {
        'completeness': 98.5,
        'duplicates_removed': 500,
        'outliers_detected': 120
    },
    'outputs_generated': {
        'pdf_report': True,
        'html_report': True,
        'powerbi_exports': True,
        'visualizations': 8
    }
}
```

### Error Handling

```python
try:
    # Execute pipeline stage
    result = execute_stage()
except DatabaseConnectionError as e:
    logger.error(f"Database connection failed: {e}")
    # Retry logic
    retry_with_backoff(execute_stage, max_retries=3)
except DataValidationError as e:
    logger.error(f"Data validation failed: {e}")
    # Send alert
    send_alert('Data Quality Issue', str(e))
    # Continue with partial data
    result = execute_stage_with_fallback()
except Exception as e:
    logger.critical(f"Unexpected error: {e}")
    # Fail pipeline
    raise
```

---

## ðŸ“Š Pipeline Performance

### Benchmarks (100K records)

| Stage | Duration | Throughput | Memory |
|-------|----------|------------|--------|
| Extraction | 2.5s | 40K rec/s | 80 MB |
| Transformation | 1.8s | 55K rec/s | 120 MB |
| Analysis | 8.0s | 12K rec/s | 160 MB |
| Reporting | 3.1s | - | 40 MB |
| **Total** | **15.4s** | **6.5K rec/s** | **160 MB peak** |

### Optimization Opportunities

1. **Parallel Processing**: Run EDA, Segmentation, ROI in parallel
2. **Incremental Updates**: Only process new data
3. **Caching**: Cache computed metrics
4. **Database**: Read replicas for load distribution

---

## ðŸ”„ Pipeline Orchestration

### Manual Execution
```bash
./run_analysis.sh
```

### Scheduled Execution (Future)
```yaml
# Apache Airflow DAG
dag = DAG(
    'marketing_analytics_pipeline',
    default_args=default_args,
    schedule_interval='0 6 * * *',  # Daily at 6 AM
)

extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag
)

analyze_task = PythonOperator(
    task_id='analyze_data',
    python_callable=analyze_data,
    dag=dag
)

report_task = PythonOperator(
    task_id='generate_reports',
    python_callable=generate_reports,
    dag=dag
)

extract_task >> transform_task >> analyze_task >> report_task
```

---

**Document Version**: 1.0  
**Last Updated**: November 4, 2025  
**Maintained By**: Data Engineering Team

